{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a supervised learning algorithm used for binary classification tasks. It predicts the probability that a given input belongs to a particular class.\n",
    "\n",
    "## Sigmoid for Classification\n",
    "- Logistic regression uses the **sigmoid** function to convert linear predictions into probabilities:\n",
    "  \n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "- It maps any real value \\(z\\) to the range \\((0, 1)\\), suitable for probabilities.\n",
    "- Decision boundary typically set at probability = 0.5.\n",
    "\n",
    "## Initialization of Weights and Bias\n",
    "- Weights \\(W\\) initialized with small random numbers to avoid saturating the sigmoid function:\n",
    "\n",
    "```python\n",
    "W = np.random.randn(1, n_features) * 0.01\n",
    "b = 0\n",
    "\n",
    "```\n",
    "\n",
    "Symmetry Problem: If all weights are initialized identically (e.g., zeros), neurons update symmetrically, causing ineffective learning. Random initialization breaks this symmetry, enabling effective learning.\n",
    "\n",
    "## Forward Propagation\n",
    "- Compute linear combination and activation:\n",
    "\n",
    "\\[\n",
    "Z = W X + b \\\\\n",
    "A = \\sigma(Z)\n",
    "\\]\n",
    "\n",
    "- Forward propagation calculates predictions \\(A\\) given current weights and bias.\n",
    "- \\(Z\\) is the linear combination of inputs and weights, transformed by the sigmoid activation to yield predicted probabilities \\(A\\).\n",
    "\n",
    "## Back Propagation\n",
    "- Computes gradients (partial derivatives) for updating weights and biases by propagating the loss backward:\n",
    "\n",
    "\\[\n",
    "dZ = A - Y \\\\\n",
    "dW = \\frac{1}{m}(dZ \\cdot X^T) \\\\\n",
    "db = \\frac{1}{m}\\sum{dZ}\n",
    "\\]\n",
    "\n",
    "- Gradients represent how the loss changes concerning each parameter.\n",
    "- \\(dZ\\) is derived by applying the chain rule to the loss function, resulting in a simple difference between predictions and actual labels.\n",
    "\n",
    "\n",
    "#TODO: compute partial derivates\n",
    "\n",
    "## Binary Cross-Entropy Loss\n",
    "- Evaluates the accuracy of predicted probabilities against actual binary labels:\n",
    "\n",
    "\\[\n",
    "L(\\hat{y}, y) = - \\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})]\n",
    "\\]\n",
    "\n",
    "- Minimizing this loss pushes predictions closer to true labels.\n",
    "\n",
    "## Regularization\n",
    "- Prevents overfitting by penalizing overly complex models (large weights):\n",
    "\n",
    "**L2 Regularization:**\n",
    "\\[\n",
    "L_{regularized} = L + \\frac{\\lambda}{2m}\\sum{W^2}\n",
    "\\]\n",
    "\n",
    "- \\(\\lambda\\) adjusts the amount of regularization.\n",
    "\n",
    "**Dropout:**\n",
    "TODO:\n",
    "\n",
    "## Gradient Descent\n",
    "- Updates parameters iteratively to minimize loss:\n",
    "\n",
    "\\[\n",
    "W = W - \\alpha \\cdot dW \\\\\n",
    "b = b - \\alpha \\cdot db\n",
    "\\]\n",
    "\n",
    "- Learning rate \\(\\alpha\\) controls how quickly the model updates. Smaller values ensure stable convergence; larger values speed up training but may cause instability.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
